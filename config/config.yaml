##define training pipeline and their parts where we need to create/store artifacts and data and from where we are getting data.
#we are defining these key in config_entity because we can't call these directly into data ingestion file.


training_pipeline_config:
  pipeline_name: visa                     #where you want to store the artifacts
  artifact_dir: artifact                  #to store the data prelated files/data files

data_ingestion_config:
  dataset_download_url: https://raw.githubusercontent.com/SiddharthTyagi119/Datasets/main/Visadataset.csv  #to download data
  raw_data_dir: raw_data               #data will store here first after download
  ingested_dir: ingested_data          #inside this we will get train and test folder for train and test data
  ingested_train_dir: train
  ingested_test_dir: test


data_validation_config:
  schema_dir: config
  schema_file_name: schema.yaml

data_transformation_config:
  transformed_dir: transformed_data
  transformed_train_dir: train
  transformed_test_dir: test
  preprocessing_dir: preprocessed
  preprocessed_object_file_name: preprocessed.pkl


model_trainer_config:
  trained_model_dir: trained_model
  model_file_name: model.pkl
  base_accuracy: 0.6
  model_config_dir: config
  model_config_file_name: model.yaml

model_evaluation_config:
  model_evaluation_file_name: model_evaluation.yaml    #saving best model path pkl file

model_pusher_config:
  model_export_dir: saved_models    #to save the best model after evaluation
























#to validate the data-> data validation will check inside ingested data how many categorical and numerical 
#data it have and store it under schema.yaml file. we can say it is a validation report.
#first of all it will get the data from data ingestion and in data validation it will check whatever dtype we
#have define at our end  is the same dtype data that we are getting, if yes then it will capture everything 
#and store in schema.yaml file. 


